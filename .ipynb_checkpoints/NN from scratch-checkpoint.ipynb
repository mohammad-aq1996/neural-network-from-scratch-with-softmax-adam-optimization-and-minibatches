{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b047bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05f9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "        Loading dataset and returning train_images, train_labels, test_images, test_labels\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv('F:\\\\TF\\\\Callbacks\\\\datasets\\\\fashion_mnist\\\\fashion-mnist_train.csv', delimiter = ',')\n",
    "    df_test = pd.read_csv('F:\\\\TF\\\\Callbacks\\\\datasets\\\\fashion_mnist\\\\fashion-mnist_test.csv', delimiter = ',')\n",
    "    train_images = df_train[df_train.columns[1:]].values\n",
    "    train_labels = df_train['label']   \n",
    "    test_images = df_test[df_test.columns[1:]].values \n",
    "    test_labels = df_test['label']\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "x_train, y_train, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7be816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X):\n",
    "    \"\"\"\n",
    "    Normalizing the inputs\n",
    "    \n",
    "    Argument:\n",
    "    X --> Input array\n",
    "    \n",
    "    Returns:\n",
    "    normal --> Normalized input\n",
    "    \"\"\"\n",
    "    normal = X / 255.\n",
    "    return normal.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce005c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, c):\n",
    "    \"\"\"\n",
    "    Turning labels to one hot matrix\n",
    "    \n",
    "    Argument:\n",
    "        y--> label/ground truth.\n",
    "        c--> Number of classes.\n",
    "    Returns:\n",
    "        One hot label\n",
    "    \"\"\"\n",
    "    # A zero matrix of size (m, c)\n",
    "    y_hot = np.zeros((len(y), c))\n",
    "    \n",
    "    # Putting 1 for column where the label is,\n",
    "    # Using multidimensional indexing.\n",
    "    y_hot[np.arange(len(y)), y] = 1\n",
    "    \n",
    "    return y_hot.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8c0edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = one_hot(y_train, 10)\n",
    "y_test = one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd6dcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb199c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocessing(x_train)\n",
    "x_test = preprocessing(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c7087ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9563037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_tanh(x):\n",
    "    return (1 - np.power(np.tanh(x), 2))\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.array(x > 0, dtype = np.float32)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return expX/np.sum(expX, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "015081b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_paramaters_with_He(n_x, n_h1, n_h2, n_y):\n",
    "    \"\"\"\n",
    "    Initializing parameters using He initialization\n",
    "    \n",
    "    Argumens:\n",
    "        n_x -- size of the input layer\n",
    "        n_h1 -- size of the first hidden layer\n",
    "        n_h2 -- size of the secound output layer\n",
    "        n_y -- size of the output layer\n",
    "    Returns:\n",
    "        Parameters --> A dictionary which contains:\n",
    "                                                    W1 --> weight matrix of shape (n_h1, n_x)\n",
    "                                                    b1 --> bias vector of shape (n_h1, 1)\n",
    "                                                    W2 --> weight matrix of shape (n_h2, n_h1)\n",
    "                                                    b2 --> bias vector of shape (n_h2, 1)\n",
    "                                                    W3 --> weight matrix of shape (n_y, n_h2)\n",
    "                                                    b3 --> bias vector of shape (n_y, 1)\n",
    "                                                    \n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    \n",
    "    parameters['W1'] = np.random.randn(n_h1, n_x) * np.sqrt(2./n_x)\n",
    "    parameters['b1'] = np.zeros((n_h1, 1))\n",
    "\n",
    "    parameters['W2'] = np.random.randn(n_h2, n_h1) * np.sqrt(2./n_h1)\n",
    "    parameters['b2'] = np.zeros((n_h2, 1))   \n",
    "\n",
    "    parameters['W3'] = np.random.randn(n_y, n_h2) * np.sqrt(2./n_h2)\n",
    "    parameters['b3'] = np.zeros((n_y, 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddb33f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 16):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "        X --> Input data with shapes of:(input size, number of examples)\n",
    "        Y --> Labels with shape of:(number of class, number of example)\n",
    "        mini_batch_size --> size of the mini-batches\n",
    "        \n",
    "    Returns:\n",
    "        list of (mini_batch_X, mini_batch_Y) \n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "        \n",
    "#     1: Shuffling (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "    \n",
    "#     2. Partition (shuffled_X, shuffled_Y)\n",
    "    num_complete_minibatches = m//mini_batch_size\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "#      Handling the end case\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, int(m/mini_batch_size)*mini_batch_size : ]\n",
    "        mini_batch_Y = shuffled_Y[:, int(m/mini_batch_size)*mini_batch_size : ]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b790be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Computing forward propagation of Neural Network\n",
    "    \n",
    "    Arguments:\n",
    "        X --> Input data with shapes of:(input size, number of examples)\n",
    "        Parameters --> A dictionary which contains:\n",
    "                                                    W1 --> weight matrix of shape (n_h1, n_x)\n",
    "                                                    b1 --> bias vector of shape (n_h1, 1)\n",
    "                                                    W2 --> weight matrix of shape (n_h2, n_h1)\n",
    "                                                    b2 --> bias vector of shape (n_h2, 1)\n",
    "                                                    W3 --> weight matrix of shape (n_y, n_h2)\n",
    "                                                    b3 --> bias vector of shape (n_y, 1)\n",
    "    Returns:\n",
    "        activation_cache --> A dictionary which contains all activations: A1, A2, A3\n",
    "    \"\"\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']    \n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    Z3 = np.dot(W3,A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    \n",
    "    activation_cache = {'A1': A1, 'A2': A2, 'A3': A3}\n",
    "    \n",
    "    return activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3267023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Computing cost function\n",
    "    \n",
    "    Arguments:\n",
    "        AL --> The softmax output of the last activation, of shape (number of class, number of examples)\n",
    "        Y --> Labels with shape of:(number of class, number of example)\n",
    "    Returns:\n",
    "        cost--> Categorical Cross Entropy\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(Y*np.log(AL)) * (1/m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5c809bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, caches, parameters):\n",
    "    \"\"\"\n",
    "    Computing Back propagation of Nural Network\n",
    "    \n",
    "    Arguments:\n",
    "        X --> Input data with shapes of:(input size, number of examples)\n",
    "        Y --> Labels with shape of:(number of class, number of example)\n",
    "        caches --> A dictionary which contains all activations: A1, A2, A3\n",
    "        Parameters --> A dictionary which contains:\n",
    "                                                    W1 --> weight matrix of shape (n_h1, n_x)\n",
    "                                                    b1 --> bias vector of shape (n_h1, 1)\n",
    "                                                    W2 --> weight matrix of shape (n_h2, n_h1)\n",
    "                                                    b2 --> bias vector of shape (n_h2, 1)\n",
    "                                                    W3 --> weight matrix of shape (n_y, n_h2)\n",
    "                                                    b3 --> bias vector of shape (n_y, 1)\n",
    "        \n",
    "    Returns:\n",
    "        grads --> A dictionary containing gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']    \n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    A1, A2, A3 = caches['A1'], caches['A2'], caches['A3']\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1/m) * np.dot(dZ3, A2.T)\n",
    "    db3 = (1/m) * np.sum(dZ3, keepdims=True, axis=1)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = dA2 * derivative_relu(A2)\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, keepdims=True, axis=1)\n",
    "\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * derivative_relu(A1)\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, keepdims=True, axis=1)\n",
    "    grads = {'dZ3': dZ3, 'dW3': dW3, 'db3': db3,\n",
    "              'dA2': dA2, 'dZ2': dZ2, 'dW2': dW2, 'db2': db2,\n",
    "              'dA1': dA1, 'dZ1': dZ1, 'dW1': dW1, 'db1': db1}\n",
    "             \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4260b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Updating parameters using Gradient Desent\n",
    "    \n",
    "    Arguments:\n",
    "        Parameters --> A dictionary which contains:\n",
    "                                                    W1 --> weight matrix of shape (n_h1, n_x)\n",
    "                                                    b1 --> bias vector of shape (n_h1, 1)\n",
    "                                                    W2 --> weight matrix of shape (n_h2, n_h1)\n",
    "                                                    b2 --> bias vector of shape (n_h2, 1)\n",
    "                                                    W3 --> weight matrix of shape (n_y, n_h2)\n",
    "                                                    b3 --> bias vector of shape (n_y, 1)\n",
    "        grads --> A dictionary containing gradients with respect to different parameters\n",
    "        learning_rate --> The learning Rate\n",
    "    Returns:\n",
    "        parameters --> A dictionary which contans updated parameters \n",
    "    \"\"\"  \n",
    "    parameters['W1'] = parameters['W1'] - learning_rate * grads['dW1']\n",
    "    parameters['b1'] = parameters['b1'] - learning_rate * grads['db1']\n",
    "    \n",
    "    parameters['W2'] = parameters['W2'] - learning_rate * grads['dW2']\n",
    "    parameters['b2'] = parameters['b2'] - learning_rate * grads['db2']    \n",
    "    \n",
    "    parameters['W3'] = parameters['W3'] - learning_rate * grads['dW3']\n",
    "    parameters['b3'] = parameters['b3'] - learning_rate * grads['db3']\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55f96950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_adam(parameters):\n",
    "    \"\"\"\n",
    "    Initializaing exponentially weighted averages with zero\n",
    "    \n",
    "    Arguments:\n",
    "    parameters --> A dictionary which contains:\n",
    "                                                    W1 --> weight matrix of shape (n_h1, n_x)\n",
    "                                                    b1 --> bias vector of shape (n_h1, 1)\n",
    "                                                    W2 --> weight matrix of shape (n_h2, n_h1)\n",
    "                                                    b2 --> bias vector of shape (n_h2, 1)\n",
    "                                                    W3 --> weight matrix of shape (n_y, n_h2)\n",
    "                                                    b3 --> bias vector of shape (n_y, 1)\n",
    "    \n",
    "    Returns: \n",
    "    v --> python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "    s --> python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "    \"\"\"\n",
    "    v = {}\n",
    "    s = {}\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']    \n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    v['dW1'] = np.zeros((W1.shape[0], W1.shape[1]))\n",
    "    v['db1'] = np.zeros((b1.shape[0], b1.shape[1]))  \n",
    "\n",
    "    v['dW2'] = np.zeros((W2.shape[0], W2.shape[1]))\n",
    "    v['db2'] = np.zeros((b2.shape[0], b2.shape[1]))\n",
    "\n",
    "    v['dW3'] = np.zeros((W3.shape[0], W3.shape[1]))\n",
    "    v['db3'] = np.zeros((b3.shape[0], b3.shape[1]))\n",
    "\n",
    "    #S\n",
    "    s['dW1'] = np.zeros((W1.shape[0], W1.shape[1]))\n",
    "    s['db1'] = np.zeros((b1.shape[0], b1.shape[1]))  \n",
    "\n",
    "    s['dW2'] = np.zeros((W2.shape[0], W2.shape[1]))\n",
    "    s['db2'] = np.zeros((b2.shape[0], b2.shape[1]))\n",
    "\n",
    "    s['dW3'] = np.zeros((W3.shape[0], W3.shape[1]))\n",
    "    s['db3'] = np.zeros((b3.shape[0], b3.shape[1]))\n",
    "\n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35279ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_adam(parameters, grads, learning_rate, v, s, t, beta1=0.9, beta2=0.999, epcilon=1e-8):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam optimizer\n",
    "    \n",
    "    Arguments:\n",
    "    parameters --> A dictionary which contains:\n",
    "                                                    W1 --> weight matrix of shape (n_h1, n_x)\n",
    "                                                    b1 --> bias vector of shape (n_h1, 1)\n",
    "                                                    W2 --> weight matrix of shape (n_h2, n_h1)\n",
    "                                                    b2 --> bias vector of shape (n_h2, 1)\n",
    "                                                    W3 --> weight matrix of shape (n_y, n_h2)\n",
    "                                                    b3 --> bias vector of shape (n_y, 1)\n",
    "    grads --> grads --> A dictionary containing gradients with respect to different parameters\n",
    "    v --> Adam variable, moving average of the first gradient, python dictionary\n",
    "    s --> Adam variable, moving average of the squared gradient, python dictionary\n",
    "    learning_rate -- the learning rate\n",
    "    beta1 --> Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 --> Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon --> hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters --> python dictionary containing your updated parameters \n",
    "    v --> Adam variable, moving average of the first gradient, python dictionary\n",
    "    s --> Adam variable, moving average of the squared gradient, python dictionary\n",
    "    \"\"\"\n",
    "    v['dW1'] = beta1 * v['dW1'] + (1-beta1) * grads['dW1']\n",
    "    v['db1'] = beta1 * v['db1'] + (1-beta1) * grads['db1']\n",
    "    v['dW2'] = beta1 * v['dW2'] + (1-beta1) * grads['dW2']\n",
    "    v['db2'] = beta1 * v['db2'] + (1-beta1) * grads['db2']\n",
    "    v['dW3'] = beta1 * v['dW3'] + (1-beta1) * grads['dW3']\n",
    "    v['db3'] = beta1 * v['db3'] + (1-beta1) * grads['db3']\n",
    "\n",
    "    s['dW1'] = beta1 * s['dW1'] + (1-beta2) * np.square(grads['dW1'])\n",
    "    s['db1'] = beta1 * s['db1'] + (1-beta2) * np.square(grads['db1'])\n",
    "    s['dW2'] = beta1 * s['dW2'] + (1-beta2) * np.square(grads['dW2'])\n",
    "    s['db2'] = beta1 * s['db2'] + (1-beta2) * np.square(grads['db2'])\n",
    "    s['dW3'] = beta1 * s['dW3'] + (1-beta2) * np.square(grads['dW3'])\n",
    "    s['db3'] = beta1 * s['db3'] + (1-beta2) * np.square(grads['db3'])\n",
    "\n",
    "    VdW1_corrected = v['dW1'] / (1-beta1**t)\n",
    "    Vdb1_corrected = v['db1'] / (1-beta1**t)  \n",
    "    VdW2_corrected = v['dW2'] / (1-beta1**t)\n",
    "    Vdb2_corrected = v['db2'] / (1-beta1**t)  \n",
    "    VdW3_corrected = v['dW3'] / (1-beta1**t)\n",
    "    Vdb3_corrected = v['db3'] / (1-beta1**t)\n",
    "\n",
    "    SdW1_corrected = s['dW1'] / (1-beta2**t)\n",
    "    Sdb1_corrected = s['db1'] / (1-beta2**t)  \n",
    "    SdW2_corrected = s['dW2'] / (1-beta2**t)\n",
    "    Sdb2_corrected = s['db2'] / (1-beta2**t)  \n",
    "    SdW3_corrected = s['dW3'] / (1-beta2**t)\n",
    "    Sdb3_corrected = s['db3'] / (1-beta2**t)\n",
    "\n",
    "    parameters['W1'] = parameters['W1'] - learning_rate * VdW1_corrected/(np.sqrt(SdW1_corrected)+epcilon)\n",
    "    parameters['b1'] = parameters['b1'] - learning_rate * Vdb1_corrected/(np.sqrt(Sdb1_corrected)+epcilon)\n",
    "\n",
    "    parameters['W2'] = parameters['W2'] - learning_rate * VdW2_corrected/(np.sqrt(SdW2_corrected)+epcilon)\n",
    "    parameters['b2'] = parameters['b2'] - learning_rate * Vdb2_corrected/(np.sqrt(Sdb2_corrected)+epcilon)    \n",
    "\n",
    "    parameters['W3'] = parameters['W3'] - learning_rate * VdW3_corrected/(np.sqrt(SdW3_corrected)+epcilon)\n",
    "    parameters['b3'] = parameters['b3'] - learning_rate * Vdb3_corrected/(np.sqrt(Sdb3_corrected)+epcilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fe00246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computing the accuracy of th model\n",
    "    Arguments:\n",
    "        X --> Input data with shapes of:(input size, number of examples)\n",
    "        Y --> Labels with shape of:(number of class, number of example)\n",
    "        parameters --> python dictionary containing your updated parameters \n",
    "    Returns:\n",
    "        accuracy --> Accuracy of your model       \n",
    "    \"\"\"\n",
    "    forward_cache = forward_propagation(X, parameters)\n",
    "    a_out = forward_cache['A3']   # containes propabilities with shape(10, 1)\n",
    "    \n",
    "    a_out = np.argmax(a_out, 0)  # 0 represents row wise \n",
    "    \n",
    "    Y = np.argmax(Y, 0)\n",
    "    \n",
    "    accuracy = np.mean(a_out == Y)*100\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "219e1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X ,Y, epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    3 Layers Neural Network which run in Adam optimizer\n",
    "    \n",
    "    Arguments:\n",
    "        X --> Input data with shapes of:(input size, number of examples)\n",
    "        Y --> Labels with shape of:(number of class, number of example)\n",
    "        epochs --> Number of epochs\n",
    "        learning_rate --> The learning rate\n",
    "    Return:\n",
    "        parameters --> python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    t = 0\n",
    "    m = Y.shape[1]\n",
    "    n_X = X.shape[0]\n",
    "    n_Y = Y.shape[0]\n",
    "    parameters = initialization_paramaters_with_He(n_X, 16, 8, n_Y)\n",
    "    v, s = init_adam(parameters)\n",
    "    for i in range(epochs):\n",
    "\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size=16)\n",
    "        cost_total = 0\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # learning_rate = 0.98**i\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "\n",
    "            forward_cache = forward_propagation(minibatch_X, parameters)\n",
    "              \n",
    "            cost_total += compute_cost(forward_cache['A3'], minibatch_Y)\n",
    "              \n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, forward_cache, parameters)\n",
    "              \n",
    "            # parameters = update_parameters(parameters, grads, learning_rate)\n",
    "            t += 1\n",
    "            parameters, v, s = update_with_adam(parameters, grads, learning_rate, v, s, t)\n",
    "\n",
    "        cost = cost_total/m    \n",
    "        if i%1 == 0:\n",
    "            print(f'After epoch {i} cost is: {cost}')\n",
    "        \n",
    "        costs.append(cost)\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "179784b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 0 cost is: 0.04693503200829832\n",
      "After epoch 1 cost is: 0.028440090967323677\n",
      "After epoch 2 cost is: 0.02639334393332623\n",
      "After epoch 3 cost is: 0.02519523746914222\n",
      "After epoch 4 cost is: 0.024440557101020454\n",
      "After epoch 5 cost is: 0.023951471003743546\n",
      "After epoch 6 cost is: 0.023484503529305215\n",
      "After epoch 7 cost is: 0.023217397344790325\n",
      "After epoch 8 cost is: 0.022965805369106407\n",
      "After epoch 9 cost is: 0.022996172455248785\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdTklEQVR4nO3da3BcZ53n8e9frUtLsqTuxJKtW8uBKHYcQ6xeb8hOagZwYMcJFIYXu5VQXCa7tSG1ZAhTU0WFecUUbyhqYEimUkkFJrvJQpGigKnJZF0TWCemdthKiG8x8S0Wji+yZUsK1sWWZN3++6KP5LasWC27rdPd5/ep6lKfc57T/e8uWz+d5znPOebuiIhI9JSFXYCIiIRDASAiElEKABGRiFIAiIhElAJARCSiysMuYClWrlzpa9asCbsMEZGismvXrgF3b5y/vqgCYM2aNezcuTPsMkREioqZHV9ovbqAREQiSgEgIhJRCgARkYhSAIiIRJQCQEQkohQAIiIRpQAQEYmoSATAa4f6eOq17rDLEBEpKJEIgP/3hwGe2H6Ei1PTYZciIlIwIhEA6VSSiakZDpweDrsUEZGCEY0A6EgCsPvEYLiFiIgUkEgEwKr6OC0NcXafOBd2KSIiBSMSAQDQ1ZFkr44ARETmRCYA0qkkpwbHODs8HnYpIiIFIUIBkABg93F1A4mIQIQCYH1LPZWxMo0DiIgEIhMAVeUxNrTWs0fjACIiQIQCADLjAPtODTExNRN2KSIioYtWAHQEE8J6NSFMRCRSAdClgWARkTmRCoDmhmqaG+LsOTkYdikiIqHLKQDMbIuZHTazbjN7fIHtZmZPBtv3mVl63vaYme0xs5ez1n3LzE6Z2d7gcf/1f5zFpVNJHQGIiJBDAJhZDHgKuA9YDzxoZuvnNbsP6AweDwNPz9v+GHBwgZf/e3ffGDy2LbX4a9GVSnBqcIw+TQgTkYjL5QjgLqDb3Y+6+wTwIrB1XputwAue8TqQMLNmADNrAz4F/CiPdV+zSxeG01GAiERbLgHQCpzMWu4J1uXa5gfAN4CFzr18NOgyes7Mkgu9uZk9bGY7zWxnf39/DuVe3R3BhDDNBxCRqMslAGyBdZ5LGzP7NNDn7rsW2P408EFgI9ALfG+hN3f3Z919k7tvamxszKHcq6sqj3FHa72OAEQk8nIJgB6gPWu5DTidY5t7gM+Y2TEyXUebzezHAO5+1t2n3X0G+CGZrqZlkU4l2dejCWEiEm25BMCbQKeZ3WJmlcADwEvz2rwEfCk4G+huYMjde939m+7e5u5rgv1edfcvAMyOEQQ+B7x9vR8mV+lUkotTMxzUhDARibDyxRq4+5SZPQq8AsSA59x9v5k9Emx/BtgG3A90A6PAQzm893fNbCOZ7qRjwFeu5QNci9kJYXtOnOPO9sRyva2ISEFZNAAAglM0t81b90zWcwe+ushr7AB2ZC1/cQl15lVLoprV9XF2nxjkL+4JqwoRkXBFaiZwtnRHQgPBIhJp0Q2AVJKec2P0jWhCmIhEU2QD4NI4wGCodYiIhCWyAXBHSwMVMVM3kIhEVmQDIF4R446WBvYcHwy7FBGRUEQ2AGD2DmGDTE5rQpiIRE+kA6ArlWB8UhPCRCSaIh0As1cG1UCwiERRpAOgpSHOqvoqDQSLSCRFOgDMLHOHMAWAiERQpAMAMuMAJ/84Rv/IxbBLERFZVpEPgHRqdhxARwEiEi2RD4ANrbMTwgbDLkVEZFlFPgDiFTHWtzRoHEBEIifyAQDQ1Z5gX48mhIlItCgAyMwHGJ+c4fCZkbBLERFZNgoAIB1cGVTdQCISJQoAoDVRTVNdFbuPKwBEJDoUAGQmhHWlEjoTSEQiRQEQSKeSnPjjKAPnNSFMRKJBARDQheFEJGoUAIEPtTZQXqY7hIlIdCgAApk7hNVrIFhEIkMBkKUrlWRfzxBTmhAmIhGgAMjSlUowNjnNIU0IE5EIUABk0ZVBRSRKFABZ2pLVNNZVaT6AiESCAiCLmdHVntARgIhEggJgnnRHkmPvjfKeJoSJSIlTAMxzaRxgMNxCRERuMAXAPJoQJiJRoQCYp7oyxu3N9QoAESl5OQWAmW0xs8Nm1m1mjy+w3czsyWD7PjNLz9seM7M9ZvZy1rqbzOzXZnYk+Jm8/o+TH+lUQhPCRKTkLRoAZhYDngLuA9YDD5rZ+nnN7gM6g8fDwNPztj8GHJy37nFgu7t3AtuD5YKQ7kgyOjHN4bOaECYipSuXI4C7gG53P+ruE8CLwNZ5bbYCL3jG60DCzJoBzKwN+BTwowX2eT54/jzw2Wv7CPk3OxCs+QAiUspyCYBW4GTWck+wLtc2PwC+AczvT1nl7r0Awc+mhd7czB42s51mtrO/vz+Hcq9fW7KalSsq2aMLw4lICcslAGyBdZ5LGzP7NNDn7ruWXNnsi7g/6+6b3H1TY2Pjtb7MkmTuEJZkz8nBZXk/EZEw5BIAPUB71nIbcDrHNvcAnzGzY2S6jjab2Y+DNmezuomagb4lV38DpVNJ3h24wB8vTIRdiojIDZFLALwJdJrZLWZWCTwAvDSvzUvAl4Kzge4Ghty9192/6e5t7r4m2O9Vd/9C1j5fDp5/Gfjn6/0w+ZROJQBdGE5ESteiAeDuU8CjwCtkzuT5mbvvN7NHzOyRoNk24CjQDfwQ+O85vPd3gE+a2RHgk8FywfhQWwMxTQgTkRJWnksjd99G5pd89rpnsp478NVFXmMHsCNr+T3g3txLXV41leXc3lynS0KISMnSTOCrSKeSvHVykOmZ+WPeIiLFTwFwFelUkgsT0xzWHcJEpAQpAK6iKxgI1jiAiJQiBcBVpG6q4ebaSo0DiEhJUgBcxdyEMB0BiEgJUgAsIt2R4OjABc5pQpiIlBgFwCLm7hB2UkcBIlJaFACL+HAwIUzjACJSahQAi6ipLGfd6jqdCSQiJUcBkIN0KsneE5oQJiKlRQGQg3RHggsT07yjO4SJSAlRAOSgqz0YCNY4gIiUEAVADjpuruGm2kqNA4hISVEA5MDMSKcSCgARKSkKgBx1pZIc7b/A4KgmhIlIaVAA5Gj2wnC6T7CIlAoFQI7ubEtQZrDnuLqBRKQ0KAByVFtVzrrV9ezWmUAiUiIUAEuQ7kiwV3cIE5ESoQBYgq72JOcvTnGkTxPCRKT4KQCWIN2hCWEiUjoUAEuwZnZCmAaCRaQEKACWwMzoateEMBEpDQqAJepKJfiDJoSJSAlQACzR7B3C9mpCmIgUOQXAEt3ZnpkQpvkAIlLsFABLVFtVztrV9ezROICIFDkFwDXoSiXYe2KQGU0IE5EipgC4BulUkpGLU3T3nw+7FBGRa6YAuAbp4Mqgmg8gIsVMAXANbllZS6KmQvMBRKSoKQCuwaUJYYNhlyIics0UANconUrS3XeeobHJsEsREbkmOQWAmW0xs8Nm1m1mjy+w3czsyWD7PjNLB+vjZvY7M3vLzPab2d9m7fMtMztlZnuDx/35+1g33uyF4TQhTESK1aIBYGYx4CngPmA98KCZrZ/X7D6gM3g8DDwdrL8IbHb3O4GNwBYzuztrv793943BY9t1fZJlNjchTAPBIlKkcjkCuAvodvej7j4BvAhsnddmK/CCZ7wOJMysOViePVeyIniUxMnzK6rKuW1VnQaCRaRo5RIArcDJrOWeYF1ObcwsZmZ7gT7g1+7+Rla7R4Muo+fMLLnQm5vZw2a208x29vf351Du8ulKJdl7UhPCRKQ45RIAtsC6+b/x3reNu0+7+0agDbjLzDYE258GPkima6gX+N5Cb+7uz7r7Jnff1NjYmEO5yyedSjAyPsUfNCFMRIpQLgHQA7RnLbcBp5faxt0HgR3AlmD5bBAOM8APyXQ1FZXZgWB1A4lIMcolAN4EOs3sFjOrBB4AXprX5iXgS8HZQHcDQ+7ea2aNZpYAMLNq4BPAoWC5OWv/zwFvX99HWX4fWFlLQ3UFu48Phl2KiMiSlS/WwN2nzOxR4BUgBjzn7vvN7JFg+zPANuB+oBsYBR4Kdm8Gng/OJCoDfubuLwfbvmtmG8l0FR0DvpKvD7VczIyuVII9J3UEICLFZ9EAAAhO0dw2b90zWc8d+OoC++0Dut7nNb+4pEoLVDqV5Dfv9DM8Pkl9vCLsckREcqaZwNcpnUriDnt1WQgRKTIKgOt0Z3sDZhoIFpHiowC4TnXxCm5rqmOPjgBEpMgoAPIg3ZFgz4lzmhAmIkVFAZAHXakkw+NTHB3QhDARKR4KgDxIp4IJYZoPICJFRAGQBx9YWUt9vFwDwSJSVBQAeVBWZnSlkhoIFpGiogDIk3QqyTt9IwyP6w5hIlIcFAB5ku5I4A5v6Q5hIlIkFAB5cmd7IjMhTAPBIlIkFAB5Uh+voLNphS4MJyJFQwGQR+lgIFgTwkSkGCgA8iidSjI0NsnRgQthlyIisigFQB51pRKALgwnIsVBAZBHH2xcQX28XPMBRKQoKADyqKzM2JhKskdHACJSBBQAeZZOJTh8doQRTQgTkQKnAMizruAOYW+dHAq7FBGRq1IA5NnG9gSAuoFEpOApAPKsoTozIUxnAolIoVMA3ADpVJI9Jwdx14QwESlcCoAbIN2RYHBUE8JEpLApAG6AruAOYZoPICKFTAFwA9zauII63SFMRAqcAuAGKCszNrYn2H1cASAihUsBcIOkU0neOTvC+YtTYZciIrIgBcAN0pVKMOOwT3cIE5ECpQC4QbraMwPBGgcQkUKlALhBGmoquLVpBbt1JpCIFCgFwA2UTiXYc+KcJoSJSEFSANxAXakk50YnOfbeaNiliIhcIacAMLMtZnbYzLrN7PEFtpuZPRls32dm6WB93Mx+Z2Zvmdl+M/vbrH1uMrNfm9mR4Gcyfx+rMKSDCWE6HVRECtGiAWBmMeAp4D5gPfCgma2f1+w+oDN4PAw8Hay/CGx29zuBjcAWM7s72PY4sN3dO4HtwXJJ6WxaQV2VJoSJSGHK5QjgLqDb3Y+6+wTwIrB1XputwAue8TqQMLPmYPl80KYieHjWPs8Hz58HPnsdn6MgZe4QltBAsIgUpFwCoBU4mbXcE6zLqY2ZxcxsL9AH/Nrd3wjarHL3XoDgZ9NCb25mD5vZTjPb2d/fn0O5haWrPcHhM8OaECYiBSeXALAF1s0/reV927j7tLtvBNqAu8xsw1IKdPdn3X2Tu29qbGxcyq4FoasjmZkQ1jMYdikiIpfJJQB6gPas5Tbg9FLbuPsgsAPYEqw6a2bNAMHPvlyLLibpdl0ZVEQKUy4B8CbQaWa3mFkl8ADw0rw2LwFfCs4GuhsYcvdeM2s0swSAmVUDnwAOZe3z5eD5l4F/vr6PUpgaair4YGOtzgQSkYJTvlgDd58ys0eBV4AY8Jy77zezR4LtzwDbgPuBbmAUeCjYvRl4PjiTqAz4mbu/HGz7DvAzM/uvwAngP+XvYxWWrlSSVw/14e6YLdRbJiKy/BYNAAB330bml3z2umeynjvw1QX22wd0vc9rvgfcu5Rii1U6leTnu3o4/t4oa1bWhl2OiAigmcDLIt2RAHRhOBEpLAqAZdDZVMcKTQgTkQKjAFgGsTLjzvYGdh8fDLsUEZE5CoBlkk4lOXRmmNEJTQgTkcKgAFgm6VRmQthbJ4fCLkVEBFAALJuuVALQQLCIFA4FwDJJ1FTygcZafvq7E7yy/4xuEiMioVMALKNvb91ARayMr/yvXdz/5L/xr2+fYWZGQSAi4VAALKN7bl3Jr//qz/j+f76T8clpHvnxLj71DwoCEQmHFVNXxKZNm3znzp1hl5EXU9Mz/Mu+0zy5vZt3By6wbnUdX/9EJ/9x/WrKynS5CBHJHzPb5e6brlivAAjXbBD8w/ZujioIROQGUAAUuOkZ51/eOs2T248oCEQkrxQARWKhIHjs3k7+/A4FgYhcGwVAkVEQiEi+KACK1PSM8/K+0zyx/QhH+y+wdlUdj32iky0KAhHJkQKgyCkIRORaKQBKxGwQPLn9CH8IguBr93Zy3wYFgYgsTAFQYuYHwW2rVvDYvbcpCETkCgqAEjU94/zv3/fy5PYjdPedVxCIyBUUACVuoSD42r2d3L+hWUEgEnEKgIiYnnG2/b6XJxQEIhJQAETM/CDobAqC4EPNxBQEIpGiAIio2SB4cvsRjigIRCJJARBxMzPOtrd7eeL/ZILgptpKPnpbIx9b28hHb2skUVMZdokicoO8XwCUh1GMLL+yMuPTH27h/g3N/OrAWV7Zf4Ydh/v4pz2nKDP4dx1JPr6uic3rmli7qg4zHR2IlDodAUTY9IzzVs8grx3q49VDfew/PQxAS0Ocj61rYvPaJv7k1pupqdTfCSLFTF1Asqizw+PsOJwJg387MsCFiWkqy8u4+wM3s3ltI5vXrSJ1c03YZYrIEikAZEkuTk3z5rvnePVQHzsO93F04AIAH2ysZfO6Jj6+rolNHTdRWa67iooUOgWAXJdjAxd49VAfrx3u442jf2RieoYVVeX8aedKPr6uiY+tbaSpLh52mSKyAAWA5M2Fi1P8tnuA1w738dqhfs4MjwPw4bYGPrY2M5D84dYGTTwTKRAKALkh3J0DvcPsONzPq4f62HPiHDMOK1dU8tHbmvj4ukb+tLORhuqKsEsViSwFgCyLcxcm+M07/bx2uI/fvNPP4OgksTJjU0eSzcFpprc2rdBppiLL6LoCwMy2AE8AMeBH7v6dedst2H4/MAr8hbvvNrN24AVgNTADPOvuTwT7fAv4b0B/8DJ/4+7brlaHAqC4TE3PsPfkIK8Gp5keOjMCQFuymo8HXUUb2xMkazUJTeRGuuYAMLMY8A7wSaAHeBN40N0PZLW5H/hLMgHwEeAJd/+ImTUDzUEY1AG7gM+6+4EgAM67+9/l+iEUAMWtd2iM1w5luop+2z3A2OQ0AKvr49zeXMf6lnpub8481txcq0tViOTJ9cwEvgvodvejwQu9CGwFDmS12Qq84Jk0ed3MEmbW7O69QC+Au4+Y2UGgdd6+EhHNDdV8/iMpPv+RFOOT0+w6fo63Tw1xsHeYg70j/N8jA0zNZP4gqa6IcdvqOtY3182FwrrVddTFNZYgki+5BEArcDJruYfMX/mLtWkl+OUPYGZrgC7gjax2j5rZl4CdwF+7+7n5b25mDwMPA6RSqRzKlWIQr4hxz60ruefWlXPrLk5Nc+Ts+blAONg7zLbfn+Gnv7v0T6v9pmrWN186UljfXE9bslpjCiLXIJcAWOh/1vx+o6u2MbMVwC+Ar7v7cLD6aeDbQbtvA98D/ssVL+L+LPAsZLqAcqhXilRVeYwNrQ1saG2YW+fu9A6NB6FwKRh+deAss72XdVXlrMs6Uri9uZ61q+qoroyF9ElEikMuAdADtGcttwGnc21jZhVkfvn/xN1/OdvA3c/OPjezHwIvL6lyiQQzoyVRTUuimntvXzW3fnRiisNnRuYC4UDvML/Y1cOFicy4QpnBLStrLztSuL25nlX1VTpaEAnkEgBvAp1mdgtwCngA+Py8Ni+R6c55kUz30JC79wZnB/0jcNDdv5+9Q9YYAcDngLev43NIxNRUltOVStKVSs6tm5lxTp4bDQIhEwx7Tw7y8r65nkiSNRWXHSnc3lxHZ1OdLmkhkbRoALj7lJk9CrxC5jTQ59x9v5k9Emx/BthG5gygbjKngT4U7H4P8EXg92a2N1g3e7rnd81sI5kuoGPAV/L0mSSiysqMjptr6bi5li0bmufWD49Pcqh3hAOnhzJHDGeG+fHrx7k4NQNArMxYXR+nNVFNSyJOazJzxNE6+0hW64qoUpI0EUwiaWp6hmPvXeBA7wjvnBnh1OBY5nFujDPD40zPXP7/IllTMRcKLYlq2rJCoiVRzcoVlepakoKlG8KIZCmPlXFrUx23NtXBnZdvm55xzg6Pc3o2FIJgOD04xrH3LvDb7oG5sYZZleVlc0cMLYk4rYma4EgiTluihtUNcXUzScFRAIjMEyu7NPB8xZ9MZM5MGh6bmguH+UGx43A/fSMXL9vHDJrqqq7oWmppqJ7rctL1kmS5KQBElsjMaKipoKGmgvUt9Qu2uTg1Te9g5iiiZzYkzo1xemiMt08N8av9Z5mYnrlsn7qqcpoTcZobMkcOzQ3VNDfE58KouSFOvEKntkr+KABEboCq8hhrVtayZmXtgttnZpyBCxc5PTg+1700ezTROzTO/tNDDJyfuGK/m2oraW64PCRaEvG5gFhVH6cipq4myY0CQCQEZWVGU12cpro4G9sTC7YZn5zm7PA4pwbH6B0cp3dojNND4/QOjtFzbpQ33n2PkfGpy1/XoLGu6lIwNFTTnKimpSE+93Pliirdq0EABYBIwYpXxOZOa30/5y9O0Tt4KRhOD40HRxFjHOod4dVDfYxPXt7VVBEzVtUHXUtZwdDcUE1zIhNKDdUVGrSOAAWASBFbUVVO56o6OlfVLbjd3RkcneT00BinZ48igp+9g+PsPH6OM/t65y7Cly1eUUZDdcXcoz4e/Awel9aXZ57XXGpTUxnTabFFQAEgUsLMjGRtJcnaSu5oaViwzfSMM3D+4tz4w8D5iwyNTjI8PsnQWOYxPDZF79A4h86MMDw+eUXX03zlZXZFWMwFxRUBkvW8upy6eIUuBb5MFAAiERcry3QJraqP05XjPtMzzsh4JhjmQuKywMh6Pp5pc/KPo3PrFzrimGWWObKpjwcBEs+EQn11sC5eHqy/FBjZz+vi5RoIz5ECQESWLFZmJGoqSdQs/W5u7s7oxPSlwBi9FBLZ4TEcBMzI+CSnBsc42DvJyPgkIxenWOwCBjWVMeqDMFgwRKqDbVc8z/xcjtNt3Z0Zhxl3ZtzxueeXtmW3qYuXU1We37oUACKyrMyM2qpyaqvKaW6oXvL+MzPO+YkphscyXVHDwVFGZjn7+VQmRMYnGTg/wbsDF+a2Xe0IBKAyVjZ3RFFmXPbLefaX9eW/wGeXL2+T/cs9e3mRt1/Q/3zo3/OxtU1L3/EqFAAiUlTKyizoCrq2mdPuzvjkTHCEERxpZAXJbNfW7HYHyswos8xPm/05u74sE2qz27PbzK7D5r9GdvvZ/WefL9zm1qYVef0eQQEgIhFjZlRXxqiujLGqPh52OaHSSImISEQpAEREIkoBICISUQoAEZGIUgCIiESUAkBEJKIUACIiEaUAEBGJKPPFLqpRQMysHzh+jbuvBAbyWE6x0/dxib6Ly+n7uFwpfB8d7t44f2VRBcD1MLOd7r7QPb4jSd/HJfouLqfv43Kl/H2oC0hEJKIUACIiERWlAHg27AIKjL6PS/RdXE7fx+VK9vuIzBiAiIhcLkpHACIikkUBICISUZEIADPbYmaHzazbzB4Pu56wmFm7mb1mZgfNbL+ZPRZ2TYXAzGJmtsfMXg67lrCZWcLMfm5mh4J/J/8h7JrCYmZ/Ffw/edvMfmpmJXf3mJIPADOLAU8B9wHrgQfNbH24VYVmCvhrd78duBv4aoS/i2yPAQfDLqJAPAH8q7uvA+4kot+LmbUCXwM2ufsGIAY8EG5V+VfyAQDcBXS7+1F3nwBeBLaGXFMo3L3X3XcHz0fI/OduDbeqcJlZG/Ap4Edh1xI2M6sH/gz4RwB3n3D3wVCLClc5UG1m5UANcDrkevIuCgHQCpzMWu4h4r/0AMxsDdAFvBFyKWH7AfANYCbkOgrBB4B+4H8EXWI/MrPasIsKg7ufAv4OOAH0AkPu/qtwq8q/KASALbAu0ue+mtkK4BfA1919OOx6wmJmnwb63H1X2LUUiHIgDTzt7l3ABSCSY2ZmliTTU3AL0ALUmtkXwq0q/6IQAD1Ae9ZyGyV4KJcrM6sg88v/J+7+y7DrCdk9wGfM7BiZrsHNZvbjcEsKVQ/Q4+6zR4U/JxMIUfQJ4F1373f3SeCXwJ+EXFPeRSEA3gQ6zewWM6skM5DzUsg1hcLMjEz/7kF3/37Y9YTN3b/p7m3uvobMv4tX3b3k/srLlbufAU6a2dpg1b3AgRBLCtMJ4G4zqwn+39xLCQ6Il4ddwI3m7lNm9ijwCpmR/OfcfX/IZYXlHuCLwO/NbG+w7m/cfVt4JUmB+UvgJ8EfS0eBh0KuJxTu/oaZ/RzYTebsuT2U4CUhdCkIEZGIikIXkIiILEABICISUQoAEZGIUgCIiESUAkBEJKIUACIiEaUAEBGJqP8PB9vH4RN9eWAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = model(x_train, y_train, epochs=10, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2df5a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in Train: 88.08%\n",
      "Accuracy in Test: 86.45%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy in Train: {accuracy(x_train, y_train, parameters):.2f}%')\n",
    "print(f'Accuracy in Test: {accuracy(x_test, y_test, parameters):.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
